<!DOCTYPE html>
<html lang="en">
<head>
    <title>
        UCLA NLP Seminar Series
    </title>
    <!-- Next line is for the nice mobile view -->
    <link rel="apple-touch-icon" sizes="180x180" href="https://uclanlp.github.io/nlp-seminar/icons/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="https://uclanlp.github.io/nlp-seminar/icons/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="https://uclanlp.github.io/nlp-seminar/icons/favicon-16x16.png">
    <link rel="icon" type="image/x-icon" href="https://uclanlp.github.io/nlp-seminar/icons/favicon.ico">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="https://uclanlp.github.io/nlp-seminar/style.css">
</head>
<script src="https://uclanlp.github.io/nlp-seminar/script.js"></script>


<body>

    <!-- <nav class="nav-container">
        <div class="nav-menu">
            <ul class="menu-list">
                <li class="menu-item"><a href="https://thrash-seminars.github.io/pre/">Previous</a></li>
                <li class="menu-item"><a href="https://thrash-seminars.github.io/">Home</a></li>
            </ul>
        </div>
    </nav> -->
    <header>
        <div class="header-content">
            <div class="lab-logos">
                <img src="https://web.cs.ucla.edu/~kwchang/img/uclanlp.png" alt="Kai-Wei Chang's Lab">
            </div>
            <div class="header-text">
                <h1>UCLA NLP Seminar Series</h1>
                <p>Welcome to our weekly seminar series. Below you'll find details about upcoming talks and speakers.</p>
            </div>
            <div class="lab-logos">
                <img src="https://web.cs.ucla.edu/~kwchang/img/uclanlp.png" alt="Kai-Wei Chang's Lab">
            </div>
        </div>
    </header>

    <main>
        <h2>Talk Schedule for Fall 2024</h2>
        <table class="seminar-schedule">
            <thead>
                <tr>
                    <th>Date</th>
                    <th>Speaker</th>
                    <th>Title</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>Oct 25</td>
                    <td><a href="https://www.coalas-lab.com/elisakreiss">Elisa Kreiss</a></td>
                    <td>Translating images into words: From truthful to useful</td>
                </tr>
                <tr>
                    <td>Nov 1</td>
                    <td><a href="https://jyzhao.net/">Jieyu Zhao</a></td>
                    <td>Building Accountable NLP Models for Social Good</td>
                </tr>
                <tr>
                    <td>Nov 4</td>
                    <td><a href="https://robinjia.github.io/">Robin Jia</a></td>
                    <td>Auditing, Understanding, and Leveraging Large Language Models<a href="#" class="details-link">(details)</a></td>
                </tr>
                <!-- <tr>
                    <td>Oct 24</td>
                    <td>Jieyu Zhao</td>
                    <td>Building Accountable NLP Models for Social Good <a href="#" class="details-link">(details)</a></td>
                </tr> -->
            </tbody>
        </table>

    <h2>&#128680; Happening Today </h2>
    <div class="talk"  onclick="toggleDetails(this)">
        <div class="talk-summary">
            <div class="date">
                <div class="month">NOV</div>
                <div class="day">1</div>
            </div>
            <div class="speaker-image">
                <a href="https://jyzhao.net/" target="_block">
                <img src="https://jyzhao.net/img/jyzhao.jpg" alt="Prof. Jieyu Zhao" style="max-height: 300px;"/>
                </a>
            </div>
            <div class="speaker-text">
                <h3>Building Accountable NLP Models for Social Good</h3>
                <p><img src="icons/person.png" alt="Person Icon" class="icon"><a href="https://jyzhao.net/">Jieyu Zhao</a></p>
                <p><img src="icons/clock.png" alt="Clock Icon" class="icon">November 1, 2024, 2:00 PM</p>
                <p><img src="icons/location.png" alt="Location Icon" class="icon">289, Engineering VI</p>
                <!-- <p><img src="icons/zoom.png" alt="Zoom Icon" class="icon">To Be Announced</p> -->
            </div>
            <div class="footer">
                <button class="more-details">More Details</button>
              </div>
        </div>
        <div class="talk-details">
            <p><strong>Speaker Bio:</strong> Jieyu Zhao is an assistant professor of Computer Science Department at University of Southern California where she is leading the LIME lab. Prior to that, she was an NSF Computing Innovation Fellow at University of Maryland, College Park. Jieyu received her Ph.D. from Computer Science Department, UCLA. Her research interest lies in fairness of ML/NLP models. Her research has been covered by news media such as Wires, The Daily Mail and so on. She was invited by UN-WOMEN Beijing on a panel discussion about gender equality and social responsibility.</p>
            <p><strong>Abstract:</strong> The rapid advancement of large language models (LLMs) has unlocked a myriad of possibilities for positive societal impact, ranging from enhancing accessibility and communication to supporting disaster response and public health initiatives. However, the deployment of these technologies also raises critical concerns regarding accountability, fairness, transparency, and ethical use. In this talk, I will discuss our efforts for auditing NLP models, detecting and mitigating biases, and understanding how LLMs make decisions. We hope to open the conversation to foster a community-wide effort towards more accountable and inclusive NLP practices.</p>
        </div>
    </div>

    <h2>&#128640; Upcoming Talks </h2>
    <div class="talk"  onclick="toggleDetails(this)">
        <div class="talk-summary">
            <div class="date">
                <div class="month">NOV</div>
                <div class="day">4</div>
            </div>
            <div class="speaker-image">
                <a href="https://robinjia.github.io/" target="_block">
                <img src="https://robinjia.github.io/assets/images/profile.jpg" alt="Prof. Robin Jia" style="max-height: 300px;"/>
                </a>
            </div>
            <div class="speaker-text">
                <h3>Auditing, Understanding, and Leveraging Large Language Models</h3>
                <p><img src="icons/person.png" alt="Person Icon" class="icon"><a href="https://robinjia.github.io/">Robin Jia</a></p>
                <p><img src="icons/clock.png" alt="Clock Icon" class="icon">November 4, 2024, 4:15 PM</p>
                <p><img src="icons/location.png" alt="Location Icon" class="icon">3400 Boelter Hall</p>
                <p>Co-located with CS 201 Seminar</p>
                <!-- <p><img src="icons/zoom.png" alt="Zoom Icon" class="icon">To Be Announced</p> -->
            </div>
            <div class="footer">
                <button class="more-details">More Details</button>
              </div>
        </div>
        <div class="talk-details">
            <p><strong>Speaker Bio:</strong> Robin Jia is an Assistant Professor of Computer Science at the University of Southern California. He received his Ph.D. in Computer Science from Stanford University, where he was advised by Percy Liang. He has also spent time as a visiting researcher at Facebook AI Research, working with Luke Zettlemoyer and Douwe Kiela. He is interested broadly in natural language processing and machine learning, with a focus on scientifically understanding NLP models in order to improve their reliability. Robin’s work has received best paper awards at ACL and EMNLP.</p>
            <p><strong>Abstract:</strong> The rise of large language models offers opportunities to both scientifically study these complex systems and apply them in novel ways. In this talk, I will describe my group’s recent work along these lines. First, I will discuss data watermarks, a statistically rigorous technique for auditing a language model’s training data based only on black-box model queries. Then, we will investigate how language models memorize training data: based on results from two complementary benchmarks, I will demonstrate the viability of localizing memorized data to a sparse subset of neurons. Next, I will provide a mechanistic account of how pre-trained language models use Fourier features to solve arithmetic problems, and how pre-training plays a critical role in these mechanisms. Finally, I will show how to leverage the complementary strengths of large language models and symbolic solvers to handle complex planning tasks.</p>
        </div>
    </div>

    

    <h2> Past Talks</h2>

    <div class="talk"  onclick="toggleDetails(this)">
        <div class="talk-summary">
            <div class="date">
                <div class="month">OCT</div>
                <div class="day">25</div>
            </div>
            <div class="speaker-image">
                <a href="https://www.coalas-lab.com/elisakreiss" target="_block">
                <img src="https://comm.ucla.edu/wp-content/uploads/2023/08/ElisaSept2022.png" alt="Prof. Elisa Kreiss" style="max-height: 300px;"/>
                </a>
            </div>
            <div class="speaker-text">
                <h3>Translating images into words: From truthful to useful</h3>
                <p><img src="icons/person.png" alt="Person Icon" class="icon"><a href="https://www.coalas-lab.com/elisakreiss">Elisa Kreiss</a></p>
                <p><img src="icons/clock.png" alt="Clock Icon" class="icon">October 25, 2024, 2:00 PM</p>
                <p><img src="icons/location.png" alt="Location Icon" class="icon">MAXWELL Room 57-124, Engineering IV</p>
                <p><img src="icons/zoom.png" alt="Zoom Icon" class="icon"><a href="https://ucla.zoom.us/j/95679516486?pwd=lteEoZPTNEB5jjYFjBVH5HlElhdCJX.1">Zoom Link</a></p>
            </div>
            <div class="footer">
                <button class="more-details">More Details</button>
              </div>
        </div>
        <div class="talk-details">
            <p><strong>Speaker Bio:</strong> Elisa Kreiss is an Assistant Professor of Communication at UCLA and the lab director of the Coalas (Computation and Language for Society) Lab. Previously, she completed a PhD in Linguistics at Stanford, where she was a member of Stanford’s NLP group and the Stanford Data Science Center for Open and REproducible Science (CORES). Elisa investigates how we produce and understand language situated in the visual world. Her work combines tools from natural language processing, psycholinguistics, and human-computer interaction to advance our understanding of how communicative context shapes language use. Her research has direct applications to image accessibility – the challenge of (automatically) generating image descriptions for blind and low-vision users. Elisa’s work has been supported by several Google Research Awards, the National Science Foundation, Stanford’s Human-centered AI initiative, and Stanford’s Accelerator for Learning.</p>
            <p><strong>Abstract:</strong> Developing Vision-Language Models (VLMs) that can easily translate between the linguistic and visual modality in human-like ways has many useful applications, including making visual content accessible to blind and low vision individuals, detecting misinformation, and combating visual illiteracy. While the current generation of VLMs has quickly risen to show human-level performance on many existing benchmarks, there remains a remarkable gap between these scores and how useful the models are found to be in practice. In this talk, I will present recent and ongoing work which suggests that in order to develop and understand the merit of Vision-Language Models for downstream application, we need to define tasks and evaluation metrics that assess the communicative usefulness of the generated texts. Specifically, I will focus on the challenge of generating image descriptions and argue for moving the goal post from what can be said about an image to the fundamentally pragmatic question of what should be said about it. Based on a variety of experiments with sighted and blind and low-vision participants, I will show that the pragmatic notion of contextual relevance is a core pillar of generating human-like image descriptions, provide evidence that our current tasks and evaluation tools in NLP remain unhelpful in uncovering these context effects, and present work that starts addressing this gap. Taken together, this work provides fundamental insights into how people communicate about the visual world, and shows how we can use those insights to advance VLMs for social impact, such as non-visual accessibility.</p>
        </div>
    </div>

    
    <h2>Organizing Committee</h2>
    <div class="committee-content">
        <h3>Faculty</h3>
        <div class="row">
                <div class="speaker-image">
                        <a href="https://web.cs.ucla.edu/~kwchang/" target="_block">
                            <img src="https://web.cs.ucla.edu/~kwchang/img/myphoto.jpg" style="max-height: 200px; max-width: 200px;"/>
                        </a>
                    </figure>
                    <p><b>Prof. Kai-Wei Chang</b></p>
                </div>

                <div class="speaker-image">
                    <a href="https://violetpeng.github.io/" target="_block">
                        <img src="https://violetpeng.github.io/photos/profile22.png" style="max-height: 200px; max-width: 200px;"/>
                    </a>
                </figure>
                <p><b>Prof. Nanyun Peng </b></p>
                </div>

                <div class="speaker-image">
                    <a href="https://saadiagabriel.com/" target="_block">
                        <img src="https://saadiagabriel.com/website.png" style="max-height: 200px; max-width: 200px;"/>
                    </a>
                </figure>
                <p><b>Prof. Saadia Gabriel </b></p>
                </div>

                <div class="speaker-image">
                    <a href="https://www.coalas-lab.com/elisakreiss" target="_block">
                        <img src="https://comm.ucla.edu/wp-content/uploads/2023/08/ElisaSept2022.png" style="max-height: 200px; max-width: 200px;"/>
                    </a>
                </figure>
                <p><b>Prof. Elisa Kreiss </b></p>
            </div>

            </div>

    
        <div style="clear: both; text-align: left;">
            <h3>Students</h3>
            </div>
        <div class="row">
                <div class="speaker-image">
                        <a href="https://tanmayparekh.github.io/" target="_block">
                            <img src="https://tanmayparekh.github.io/vertical-ucla-pic.jpeg" style="max-height: 200px; max-width: 200px;"/>
                        </a>
                    </figure>
                    <p><b>Tanmay Parekh</b></p>
                </div>

                <div class="speaker-image">
                    <a href="https://yufeitian.github.io/" target="_block">
                        <img src="https://yufeitian.github.io/website/images/avatar.jpg" style="max-height: 200px; max-width: 200px;"/>
                    </a>
                </figure>
                <p><b>Yufei Tian</b></p>
                </div>

                <div class="speaker-image">
                    <a href="https://asuvarna31.github.io" target="_block">
                        <img src="https://asuvarna31.github.io/images/IMG_0888.jpg" style="max-height: 200px; max-width: 200px;"/>
                    </a>
                </figure>
                <p><b>Ashima Suvarna</b></p>
                </div>

                <div class="speaker-image">
                    <a href="https://evelinehong.github.io/" target="_block">
                        <img src="https://evelinehong.github.io/assets/images/credit_to_zeng.jpg" style="max-height: 200px; max-width: 200px;"/>
                    </a>
                </figure>
                <p><b>Yining Hong </b></p>
                </div>

                <div class="speaker-image">
                    <a href="https://salmanrahman.net/" target="_block">
                        <img src="https://salmanrahman.net/assets/salman-img.jpg" style="max-height: 200px; max-width: 150px;"/>
                    </a>
                </figure>
                <p><b>Salman Rahman </b></p>
                </div>


            </div>


    </div>

    </main>

</body>

</html>
