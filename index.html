<!DOCTYPE html>
<html lang="en">
<head>
    <title>
        UCLA NLP Seminar Series
    </title>
    <!-- Next line is for the nice mobile view -->
    <link rel="apple-touch-icon" sizes="180x180" href="https://uclanlp.github.io/nlp-seminar/icons/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="https://uclanlp.github.io/nlp-seminar/icons/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="https://uclanlp.github.io/nlp-seminar/icons/favicon-16x16.png">
    <link rel="icon" type="image/x-icon" href="https://uclanlp.github.io/nlp-seminar/icons/favicon.ico">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="https://uclanlp.github.io/nlp-seminar/style.css">
</head>
<script src="https://uclanlp.github.io/nlp-seminar/script.js"></script>


<body>

    <header>
        <div class="header-content">
            <div class="lab-logos">
                <img src="https://web.cs.ucla.edu/~kwchang/img/uclanlp.png" alt="Kai-Wei Chang's Lab">
            </div>
            <div class="header-text">
                <h1>UCLA NLP Seminar Series</h1>
                <p>Welcome to our weekly seminar series.</p>
            </div>
            <div class="lab-logos">
                <img src="https://web.cs.ucla.edu/~kwchang/img/uclanlp.png" alt="Kai-Wei Chang's Lab">
            </div>
        </div>
    </header>


    <nav class="nav-container">
              <div class="nav-menu">
            <ul class="menu-list">
                <li class="menu-item"><a href="https://uclanlp.github.io/nlp-seminar/">Home</a></li>
                <li class="menu-item"><a href="past_talks.html">Past Talks</a></li>
            </ul>
        </div>
    </nav>

    <main>
<!--         <h2>We have a great lineup of speakers for Spring 2025! Please check back in a few weeks. </h2> -->
        <table class="seminar-schedule">
            <thead>
                <tr>
                    <th>Date   </th>
                    <th>Speaker</th>
                    <th>Title</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                  <td>April 4</td>
                    <td><a href="https://homes.cs.washington.edu/~yuliats/">Yulia Tsvetkov</a></td>
                    <td>Optimizing for Long-Term Vision in a Fast-Paced Research World</td>
                </tr>
                  <tr>
                    <td>April 11</td>
                    <td><a href="https://zhegan27.github.io/">Zhe Gan</a></td>
                    <td>How to Build Your Multimodal LLMs: From Pre-training to Post-training and Agents</td>
                </tr> 
                <tr>
                    <td>April 18</td>
                    <td><a href="https://people.ischool.berkeley.edu/~dbamman/">David Bamman</a></td>
                    <td>Measuring Representation and Linguistic Variation in Hollywood</td>
                </tr> 
                 <tr>
                    <td>May 9</td>
                    <td><a href="https://adityakusupati.github.io/">Aditya Kusupati</a></td>
                    <td>Matryoshka Principles for Adaptive Intelligence</td>
                </tr>

                <tr>
                    <td>May 13</td>
                    <td><a href="https://aliceoh9.github.io/">Alice Oh</a></td>
                    <td>Beyond Accuracy: Rethinking LLM Evaluation for Real-World, Interactive, and Culturally Inclusive Scenarios</td>
                </tr>
                 <tr>
                    <td>May 16</td>
                    <td><a href="https://people.eecs.berkeley.edu/~emmapierson/">Emma Pierson</a></td>
                    <td>Using New Data to Answer Old Questions</td>
                </tr>
                 <tr>
                    <td>May 23</td>
                    <td><a href="https://www.csie.ntu.edu.tw/~sdlin/">Shou-De Lin</a> <a href="https://homepage.iis.sinica.edu.tw/pages/miyen/">Mi-Yen Yeh</a></td>
                    <td></td>
                </tr>
                <tr>
                    <td>May 30</td>
                    <td><a href="https://juliekallini.com/">Julie Kallini</a></td>
                    <td>MrT5: Dynamic Token Merging for Efficient Byte-level Language Models</td>
               </tr>
                 <tr>
                    <td>June 6</td>
                    <td><a href="https://tingofurro.github.io/">Phillipe Laban</a></td>
                    <td></td>
                </tr> 
                
            </tbody>
        </table>



<h2>&#128640; Upcoming Talks </h2>
         
    <div class="talk"  onclick="toggleDetails(this)">
        <div class="talk-summary">
            <div class="date">
                <div class="month">MAY</div>
                <div class="day">30</div>
            </div>
            <div class="speaker-image">
                <a href="https://juliekallini.com/" target="_block">
                <img src="https://juliekallini.com/assets/img/profile_pic.jpg" alt="Julie Kallini" style="max-height: 300px;"/>
                </a>
            </div>
            <div class="speaker-text">
                <h3>Talk Title</h3>
                <p><img src="icons/person.png" alt="Person Icon" class="icon"><a href="https://juliekallini.com/">Julie Kallini</a></p>
                <p><img src="icons/clock.png" alt="Clock Icon" class="icon">May 30, 2025, 2:00 PM</p>
                <p><img src="icons/location.png" alt="Location Icon" class="icon">289, Engineering VI</p>
<!--                 <p><img src="icons/zoom.png" alt="Zoom Icon" class="icon">To Be Announced</p> -->
            </div>
            <div class="footer">
                <button class="more-details">More Details</button>
              </div>
        </div>
        <div class="talk-details">
            <p><strong>Speaker Bio:</strong> Julie Kallini is a second-year Ph.D. student in Computer Science at Stanford University, advised by Christopher Potts and Dan Jurafsky. Her research focuses on natural language processing (NLP), with an emphasis on computational linguistics/cognitive science, tokenization, and model architecture. Her paper, "Mission: Impossible Language Models," won Best Paper Award at ACL 2024. Her work is supported by the NSF Graduate Research Fellowship, the Stanford School of Engineering Graduate Fellowship, and the Stanford EDGE Fellowship.
Before starting her Ph.D., Julie was a software engineer at Meta, where she worked on machine learning for advertisements. Julie graduated summa cum laude from Princeton University with a B.S.E. in Computer Science and a minor in Linguistics. </p>
            <p><strong>Abstract:</strong> Models that rely on subword tokenization have significant drawbacks, such as sensitivity to character-level noise like spelling errors and inconsistent compression rates across different languages and scripts. While character- or byte-level models like ByT5 attempt to address these concerns, they have not gained widespread adoption—processing raw byte streams without tokenization results in significantly longer sequence lengths, making training and inference inefficient. This work introduces MrT5 (MergeT5), a more efficient variant of ByT5 that integrates a token deletion mechanism in its encoder to dynamically shorten the input sequence length. MrT5 achieves up to 75% sequence length reduction with minimal performance loss, offering faster inference and competitive accuracy on multilingual and character-level tasks. Our approach presents a solution to the practical limitations of existing byte-level models. </p>
        </div>
    </div>
  

    <h2>&#128680; Past Talks </h2>

        <div class="talk"  onclick="toggleDetails(this)">
        <div class="talk-summary">
            <div class="date">
                <div class="month">MAY</div>
                <div class="day">13</div>
            </div>
            <div class="speaker-image">
                <a href="https://aliceoh9.github.io/" target="_block">
                <img src="https://aliceoh9.github.io/assets/IMG_7384.jpg" alt="Alice Oh" style="max-height: 300px;"/>
                </a>
            </div>
            <div class="speaker-text">
                <h3>Beyond Accuracy: Rethinking LLM Evaluation for Real-World, Interactive, and Culturally Inclusive Scenarios</h3>
                <p><img src="icons/person.png" alt="Person Icon" class="icon"><a href="https://aliceoh9.github.io/">Alice Oh</a></p>
                <p><img src="icons/clock.png" alt="Clock Icon" class="icon">May 13, 2025, 4:15 PM</p>
                <p><img src="icons/location.png" alt="Location Icon" class="icon">3400 Boelter Hall (Co-located with CS 201)</p>
            </div>
            <div class="footer">
                <button class="more-details">More Details</button>
              </div>
        </div>
        <div class="talk-details">
            <p><strong>Speaker Bio:</strong> Alice Oh is a Professor in the School of Computing at KAIST. Her major research area is at the intersection of natural language
processing (NLP) and computational social science, with a recent focus on multilingual and multicultural aspects of LLMs. She
collaborates with scholars in humanities and social sciences such as political science, education, and history. She has served as
Program Chair for ICLR 2021 and NeurIPS 2022, General Chair for ACM FAccT 2022 and NeurIPS 2023, and DEI Chair for
COLM 2024. She is the current President of SIGDAT which oversees EMNLP. </p>
            <p><strong>Abstract:</strong> Traditional evaluation methods for large language models (LLMs)—often centered on accuracy in static multiple-choice or short-answer questions—fail to capture the complexities of real-world use. As we envision LLMs serving users in dynamic,
multicultural, and interactive scenarios, we must rethink what meaningful evaluation looks like. This talk presents our recent
research to advance LLM evaluation through culturally aware, socially grounded, and interaction-driven benchmarks. We
assess factual consistency across languages and regions, explore everyday knowledge in underrepresented cultures,
and examine cultural inclusivity. We highlight that while LLMs may not appear to be socially biased in simple
question-answering, they reveal their biases in generation tasks, which is more aligned with the actual LLM usage. We
further introduce dynamic and interactive evaluation paradigms: LLM-as-an-Interviewer, which mimic real-time user
interaction, and Flex-TravelPlanner, which evaluates planning adaptability under evolving and prioritized constraints.
Together, these papers reveal that accuracy alone is insufficient; LLM evaluation must consider culture, context, interactivity,
and adaptation. This talk calls for a broader evaluation agenda and presents these ten papers as starting points for more
robust, inclusive, and realistic assessments. </p>
        </div>
    </div> 


         <div class="talk"  onclick="toggleDetails(this)">
        <div class="talk-summary">
            <div class="date">
                <div class="month">MAY</div>
                <div class="day">16</div>
            </div>
            <div class="speaker-image">
                <a href="https://people.eecs.berkeley.edu/~emmapierson/" target="_block">
                <img src="https://people.eecs.berkeley.edu/~emmapierson/professional_photo.jpeg" alt="Emma Pierson" style="max-height: 300px;"/>
                </a>
            </div>
            <div class="speaker-text">
                <h3>Using New Data to Answer Old Questions</h3>
                <p><img src="icons/person.png" alt="Person Icon" class="icon"><a href="https://people.eecs.berkeley.edu/~emmapierson/">Emma Pierson</a></p>
                <p><img src="icons/clock.png" alt="Clock Icon" class="icon">May 16, 2025, 2:00 PM</p>
                <p><img src="icons/location.png" alt="Location Icon" class="icon">289, Engineering VI (Virtual Speaker) </p>
              
            </div>
            <div class="footer">
                <button class="more-details">More Details</button>
              </div>
        </div>
        <div class="talk-details">
            <p><strong>Speaker Bio:</strong> Emma Pierson is an assistant professor of computer science at UC Berkeley and core faculty in the Computational Precision Health program. She develops data science and machine learning methods to study inequality and healthcare. Her work has been recognized by best paper, poster, and talk awards, an NSF CAREER award, a Rhodes Scholarship, Hertz Fellowship, Rising Star in EECS, MIT Technology Review 35 Innovators Under 35, Forbes 30 Under 30 in Science, AI2050 Early Career Fellowship, and Samsung AI Researcher of the Year. Her research has been published in venues including Nature, JAMA, The New England Journal of Medicine, PNAS, Nature Medicine, ICML and ICLR, and she has also written for The New York Times, FiveThirtyEight, Wired, and various other publications. </p>
            <p><strong>Abstract:</strong> The explosion of new data sources has created new opportunities, and necessitated new machine learning methods, to answer old questions in the health and social sciences. This talk discusses three stories under this theme: first, using image data to quantify inequality in policing; second, using text data to interpretably predict target variables and characterize disparities; and third, using address data to infer fine-grained migration patterns. </p>
        </div>
    </div> 


        <div class="talk"  onclick="toggleDetails(this)">
        <div class="talk-summary">
            <div class="date">
                <div class="month">MAY</div>
                <div class="day">9</div>
            </div>
            <div class="speaker-image">
                <a href="https://adityakusupati.github.io/" target="_block">
                <img src="https://adityakusupati.github.io/images/AdityaKusupati.jpg" alt="Aditya Kusupati" style="max-height: 300px;"/>
                </a>
            </div>
            <div class="speaker-text">
                <h3>Matryoshka Principles for Adaptive Intelligence</h3>
                <p><img src="icons/person.png" alt="Person Icon" class="icon"><a href="https://adityakusupati.github.io/">Aditya Kusupati</a></p>
                <p><img src="icons/clock.png" alt="Clock Icon" class="icon">May 09, 2025, 2:00 PM</p>
                <p><img src="icons/location.png" alt="Location Icon" class="icon">289, Engineering VI</p>
            </div>
            <div class="footer">
                <button class="more-details">More Details</button>
              </div>
        </div>
        <div class="talk-details">
            <p><strong>Speaker Bio:</strong> Aditya Kusupati is a Staff Research Scientist at Google DeepMind. He got his PhD from University of Washington and B.Tech from IIT Bombay. Between his B.Tech and PhD, he was a Research Fellow at Microsoft Research. His research focuses broadly on next-generation machine learning models geared towards adaptive intelligence. </p>
            <p><strong>Abstract:</strong> The increasing scale of deep learning models presents significant challenges for deployment across diverse computational environments, each with unique constraints on latency, memory, and energy. Traditional approaches often necessitate training and maintaining separate models for each desired operating point, leading to substantial overhead. This talk explores the "Matryoshka" principle, a promising paradigm for achieving computational adaptivity within a single trained artifact. Inspired by Russian nesting dolls, Matryoshka methods embed coarser, computationally cheaper structures within finer, more powerful ones, enabling dynamic adjustment of resource usage at inference time. This technique is highly generalizable across various fundamental components of Machine Learning like Embeddings, Transformers and even the integer data type for Quantization. The community extended it beyond just these components and has seen a wide array of deployments both across industry and open-source, serving over a Billion users daily. Collectively, these works demonstrate how the Matryoshka principle facilitates unified training of highly flexible models that can seamlessly adapt their computational footprint post-training, significantly simplifying deployment and enhancing efficiency across heterogeneous hardware. </p>
        </div>
    </div> 
        
    <div class="talk"  onclick="toggleDetails(this)">
        <div class="talk-summary">
            <div class="date">
                <div class="month">APR</div>
                <div class="day">18</div>
            </div>
            <div class="speaker-image">
                <a href="https://people.ischool.berkeley.edu/~dbamman/" target="_block">
                <img src="https://people.ischool.berkeley.edu/~dbamman/headshots/bamman_headshot_medium.jpg" alt="David Bamman" style="max-height: 300px;"/>
                </a>
            </div>
            <div class="speaker-text">
                <h3>Measuring Representation and Linguistic Variation in Hollywood</h3>
                <p><img src="icons/person.png" alt="Person Icon" class="icon"><a href="https://people.ischool.berkeley.edu/~dbamman/">Prof. David Bamman</a>, University of California Berkeley</p>
                <p><img src="icons/clock.png" alt="Clock Icon" class="icon">Apr 18, 2025, 2:00 PM</p>
                <p><img src="icons/location.png" alt="Location Icon" class="icon">289, Engineering VI</p>
<!--                 <p><img src="icons/zoom.png" alt="Zoom Icon" class="icon">To Be Announced</p> -->
            </div>
            <div class="footer">
                <button class="more-details">More Details</button>
              </div>
        </div>
        <div class="talk-details">
            <p><strong>Speaker Bio:</strong> David Bamman is an associate professor in the School of Information at UC Berkeley, where he works in the areas of natural language processing and cultural analytics, applying NLP and machine learning to empirical questions in the humanities and social sciences. His research focuses on improving the performance of NLP for underserved domains like literature (including LitBank and BookNLP) and exploring the affordances of empirical methods for the study of literature and culture. Before Berkeley, he received his PhD in the School of Computer Science at Carnegie Mellon University and was a senior researcher at the Perseus Project of Tufts University. Bamman's work is supported by the National Endowment for the Humanities, National Science Foundation, an Amazon Research Award, and an NSF CAREER award. </p>
            <p><strong>Abstract:</strong> Movies are a massively popular and influential form of media, but their computational study at scale has largely been off-limits to researchers in the United States due to the Digital Millennium Copyright Act.  In this talk, I'll discuss recent regulatory changes at the U.S. Copyright Office that allows for large-scale text and data mining of film, and describe our efforts to build a collection of 2,307 films representing the top 50 movies by U.S. box office over the period 1980 to 2022, along with award nominees.  Building this collection allows us to carry out several large-scale computational studies of film; I'll discuss our work measuring changing patterns in the representation of gender and race/ethnicity over the past 43 years (where we see an increase in diversity over the past decade) and in leveraging it to model variation in emotional performances and choice of adverbial intensifiers over both narrative and historical time. This work illustrates a new frontier of the data-driven analysis of film at a large scale </p>
        </div>
    </div> 

    

     <div class="talk"  onclick="toggleDetails(this)">
        <div class="talk-summary">
            <div class="date">
                <div class="month">APR</div>
                <div class="day">11</div>
            </div>
            <div class="speaker-image">
                <a href="https://zhegan27.github.io/" target="_block">
                <img src="https://pbs.twimg.com/profile_images/1095759122627088384/n_WL0HGz_400x400.jpg" alt="Zhe Gan" style="max-height: 300px;"/>
                </a>
            </div>
            <div class="speaker-text">
                <h3>How to Build Your Multimodal LLMs: From Pre-training to Post-training and Agents</h3>
                <p><img src="icons/person.png" alt="Person Icon" class="icon"><a href="https://zhegan27.github.io/">Zhe Gan</a>, Apple</p>
                <p><img src="icons/clock.png" alt="Clock Icon" class="icon">April 11, 2025, 2:00 PM</p>
                <p><img src="icons/location.png" alt="Location Icon" class="icon">289, Engineering VI (Virtual Speaker)</p>
<!--                 <p><img src="icons/zoom.png" alt="Zoom Icon" class="icon">To Be Announced</p> -->
            </div>
            <div class="footer">
                <button class="more-details">More Details</button>
              </div>
        </div>
        <div class="talk-details">
            <p><strong>Speaker Bio:</strong> Dr. Zhe Gan is a Research Scientist and Manager at Apple AI/ML, primarily working on building large-scale vision and multimodal foundation models. Before joining Apple, he was a Principal Researcher at Microsoft. He received his Ph.D. degree from Duke University in 2018. He has served as Area Chairs for top-tier AI conferences, and is a recipient of the Best Student Paper Honorable Mention Awards at CVPR 2021 and WACV 2021, respectively. </p>
            <p><strong>Abstract:</strong> Multimodal Large Language Models (LLMs) have become an increasing hot research topic. In this talk, I will present our recent works on how to build performant multimodal LLMs, along several fronts: (1) Pre-training, with focus on pre-training data choices, multimodal LLM pre-training and visual encoder pre-training; (2) Post-training, with focus on text-rich image understanding, visual referring and grounding, UI understanding, and reasoning; and (3) Generalist Agents, with focus on how to adapt multimodal LLMs into generalist embodied agents. </p>
        </div>
    </div> 
        
    <div class="talk"  onclick="toggleDetails(this)">
        <div class="talk-summary">
            <div class="date">
                <div class="month">APR</div>
                <div class="day">4</div>
            </div>
            <div class="speaker-image">
                <a href="https://homes.cs.washington.edu/~yuliats/" target="_block">
                <img src="https://homes.cs.washington.edu/~yuliats/yulia.jpg" alt="Yulia Tsvetkov" style="max-height: 300px;"/>
                </a>
            </div>
            <div class="speaker-text">
                <h3>Optimizing for Long-Term Vision in a Fast-Paced Research World</h3>
                <p><img src="icons/person.png" alt="Person Icon" class="icon"><a href="https://homes.cs.washington.edu/~yuliats/">Prof. Yulia Tsvetkov</a>, University of Washington</p>
                <p><img src="icons/clock.png" alt="Clock Icon" class="icon">Apr 4, 2025, 2:00 PM</p>
                <p><img src="icons/location.png" alt="Location Icon" class="icon">289, Engineering VI</p>
<!--                 <p><img src="icons/zoom.png" alt="Zoom Icon" class="icon">To Be Announced</p> -->
            </div>
            <div class="footer">
                <button class="more-details">More Details</button>
              </div>
        </div>
        <div class="talk-details">
            <p><strong>Speaker Bio:</strong> Yulia Tsvetkov is an associate professor at the Paul G. Allen School of Computer Science & Engineering at University of Washington. Her research group works on fundamental advancements to large language models, multilingual NLP, and AI ethics/safety. This research is motivated by a unified goal: to extend the capabilities of human language technology beyond individual populations and across language boundaries, thereby making NLP tools available to all users. Prior to joining UW, Yulia was an assistant professor at Carnegie Mellon University and before that a postdoc at Stanford. Yulia is a recipient of NSF CAREER, Sloan Fellowship, Okawa Research award, and multiple paper awards and runner-ups at NLP, ML, and CSS conferences.</p>
            <p><strong>Abstract:</strong> The fast-paced race for larger language models—and the promise of financial gains for the winners—incentivizes heavier engineering with incremental ideas, often at the expense of long-term vision. While this approach advances industry products used by millions, it is not necessarily the right approach for academic research. In this talk, I will present novel task formulations and evaluation benchmarks that question mainstream assumptions about LLM architectures, training/alignment algorithms, and evaluation approaches. While proposed ideas contradict the common practice, they expose blind spots in LLMs reasoning abilities, and huge performance and fairness gaps in best commercial LLMs, highlighting directions for future research.  </p>
        </div>
    </div>

        
    
        








<!--     <div class="talk"  onclick="toggleDetails(this)">
        <div class="talk-summary">
            <div class="date">
                <div class="month">JAN</div>
                <div class="day">10</div>
            </div>
            <div class="speaker-image">
                <a href="https://swabhs.com/" target="_block">
                <img src="Image Link" alt="Speaker Name" style="max-height: 300px;"/>
                </a>
            </div>
            <div class="speaker-text">
                <h3>Talk Title</h3>
                <p><img src="icons/person.png" alt="Person Icon" class="icon"><a href="Speaker Website">Speaker Name</a></p>
                <p><img src="icons/clock.png" alt="Clock Icon" class="icon">Jan 10, 2024, 2:00 PM</p>
                <p><img src="icons/location.png" alt="Location Icon" class="icon">289, Engineering VI</p>
                <p><img src="icons/zoom.png" alt="Zoom Icon" class="icon">To Be Announced</p>
            </div>
            <div class="footer">
                <button class="more-details">More Details</button>
              </div>
        </div>
        <div class="talk-details">
            <p><strong>Speaker Bio:</strong> INSERT BIO </p>
            <p><strong>Abstract:</strong> INSERT TALK ABSTRACT HERE </p>
        </div>
    </div> -->

    
    <h2>Organizing Committee</h2>
    <div class="committee-content">
        <h3>Faculty</h3>
        <div class="row">
                <div class="speaker-image">
                        <a href="https://web.cs.ucla.edu/~kwchang/" target="_block">
                            <img src="https://web.cs.ucla.edu/~kwchang/img/myphoto.jpg" style="max-height: 200px; max-width: 200px;"/>
                        </a>
                    </figure>
                    <p><b>Prof. Kai-Wei Chang</b></p>
                </div>

                <div class="speaker-image">
                    <a href="https://violetpeng.github.io/" target="_block">
                        <img src="https://violetpeng.github.io/photos/profile22.png" style="max-height: 200px; max-width: 200px;"/>
                    </a>
                </figure>
                <p><b>Prof. Nanyun Peng </b></p>
                </div>

                <div class="speaker-image">
                    <a href="https://saadiagabriel.com/" target="_block">
                        <img src="https://saadiagabriel.com/website.png" style="max-height: 200px; max-width: 200px;"/>
                    </a>
                </figure>
                <p><b>Prof. Saadia Gabriel </b></p>
                </div>

                <div class="speaker-image">
                    <a href="https://www.coalas-lab.com/elisakreiss" target="_block">
                        <img src="https://comm.ucla.edu/wp-content/uploads/2023/08/ElisaSept2022.png" style="max-height: 200px; max-width: 200px;"/>
                    </a>
                </figure>
                <p><b>Prof. Elisa Kreiss </b></p>
            </div>

            </div>

    
        <div style="clear: both; text-align: left;">
            <h3>Students</h3>
            </div>
        <div class="row">
                <div class="speaker-image">
                        <a href="https://tanmayparekh.github.io/" target="_block">
                            <img src="https://tanmayparekh.github.io/vertical-ucla-pic.jpeg" style="max-height: 200px; max-width: 200px;"/>
                        </a>
                    </figure>
                    <p><b>Tanmay Parekh</b></p>
                </div>

                <div class="speaker-image">
                    <a href="https://yufeitian.github.io/website/" target="_block">
                        <img src="https://yufeitian.github.io/website/images/avatar.jpg" style="max-height: 200px; max-width: 200px;"/>
                    </a>
                </figure>
                <p><b>Yufei Tian</b></p>
                </div>

                <div class="speaker-image">
                    <a href="https://asuvarna31.github.io" target="_block">
                        <img src="https://asuvarna31.github.io/images/IMG_0888.jpg" style="max-height: 200px; max-width: 200px;"/>
                    </a>
                </figure>
                <p><b>Ashima Suvarna</b></p>
                </div>

                <div class="speaker-image">
                    <a href="https://evelinehong.github.io/" target="_block">
                        <img src="https://evelinehong.github.io/assets/images/credit_to_zeng.jpg" style="max-height: 200px; max-width: 200px;"/>
                    </a>
                </figure>
                <p><b>Yining Hong </b></p>
                </div>

                <div class="speaker-image">
                    <a href="https://salmanrahman.net/" target="_block">
                        <img src="https://salmanrahman.net/assets/salman-img.jpg" style="max-height: 200px; max-width: 150px;"/>
                    </a>
                </figure>
                <p><b>Salman Rahman </b></p>
                </div>


            </div>


    </div>

    </main>

</body>

</html>
