<!DOCTYPE html>
<html lang="en">
<head>
    <title>
        UCLA NLP Seminar Series
    </title>
    <!-- Next line is for the nice mobile view -->
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="https://uclanlp.github.io/nlp-seminar/style.css">
</head>
<script src="https://uclanlp.github.io/nlp-seminar/script.js"></script>
<body>

    <!-- <nav class="nav-container">
        <div class="nav-menu">
            <ul class="menu-list">
                <li class="menu-item"><a href="https://thrash-seminars.github.io/pre/">Previous</a></li>
                <li class="menu-item"><a href="https://thrash-seminars.github.io/">Home</a></li>
            </ul>
        </div>
    </nav> -->
    <header>
        <div class="header-content">
            <div class="lab-logos left">
                <img src="https://web.cs.ucla.edu/~kwchang/img/uclanlp.png" alt="Kai-Wei Chang's Lab">
            </div>
            <div class="header-text">
                <h1>UCLA NLP Seminar Series</h1>
                <p>Welcome to our weekly seminar series. Below you'll find details about upcoming talks and speakers.</p>
            </div>
            <div class="lab-logos left">
                <img src="https://web.cs.ucla.edu/~kwchang/img/uclanlp.png" alt="Kai-Wei Chang's Lab">
            </div>
        </div>
    </header>

    <main>
        
    <h2>Talks in Fall 2024</h2>

    <p><b>Time:</b> Fridays @ 14:00 PT

    <p><b>Place:</b> Room 289, Engineering VI (First talk at MAXWELL Room 57-124, Engineering IV)</p>

    <!-- <p><b>Mailing list?</b> Subscribe to the seminar mailing list <a href="https://www.jiscmail.ac.uk/cgi-bin/wa-jisc.exe?A0=THRASH-SEMINARS" target="_blank">here</a>. If you want to get all the ThRaSH-related news, subscribe to <a href="https://www.jiscmail.ac.uk/cgi-bin/wa-jisc.exe?A0=THRASH" target="_blank">this list</a> as well.</p> -->

    <!-- <p><b>Organizers:</b> <a href="https://www.fim.uni-passau.de/en/intelligent-systems/team/research-staff/">Andre Opris</a> and <a href="https://www.cs.bham.ac.uk/~lehrepk/">Per Kristian Lehre</a>.</p> -->

    <!-- <h3 id="Schedule">Schedule</h3>

    <ul>
        <li><a href="#Oct22"><b>22 October: </b>Shengjie Ren, <i>A First Running Time Analysis of the Strength Pareto Evolutionary Algorithm 2 (SPEA2) </i></a></li>
        <li><a href="#Oct29"><b>29 October: </b>Alistair Benford, <i>Runtime Analysis of Coevolutionary Algorithms on a Class of Symmetric Zero-Sum Games </i></a></li>
        <li><a href="#Nov5"><b>5 November: </b>Jon Rowe, <i>Finding the Hidden Subset in Hidden Subset Problems </i></a></li>
        <li><a href="#Nov12"><b>12 November: </b>,Shishen Lin <i>Overcoming Binary Adversarial Optimisation with Competitive Coevolution </i></a></li>
    </ul> -->

    <h2>Talk Details</h2>

    <div class="talk" onclick="toggleDetails(this)">
        <div class="talk-summary">
            <div class="speaker-image">
                <img src="https://comm.ucla.edu/wp-content/uploads/2023/08/ElisaSept2022.png" alt="Prof. Elisa Kreiss" />
            </div>
            <div class="speaker-text">
                <h3>Translating images into words: From truthful to useful</h3>
                <p><img src="icons/person.png" alt="Person Icon" class="icon"><a href="https://www.coalas-lab.com/elisakreiss">Elisa Kreiss</a></p>
                <p><img src="icons/clock.png" alt="Clock Icon" class="icon">October 25, 2024, 2:00 PM</p>
                <p><img src="icons/location.png" alt="Location Icon" class="icon">MAXWELL Room 57-124, Engineering IV</p>
                <p><img src="icons/zoom.png" alt="Zoom Icon" class="icon">To be updated soon (In-person attendance encouraged)</p>
            </div>
        </div>
        <div class="talk-details">
            <p><strong>Speaker Bio:</strong> Elisa Kreiss is an Assistant Professor of Communication at UCLA and the lab director of the Coalas (Computation and Language for Society) Lab. Previously, she completed a PhD in Linguistics at Stanford, where she was a member of Stanford’s NLP group and the Stanford Data Science Center for Open and REproducible Science (CORES). Elisa investigates how we produce and understand language situated in the visual world. Her work combines tools from natural language processing, psycholinguistics, and human-computer interaction to advance our understanding of how communicative context shapes language use. Her research has direct applications to image accessibility – the challenge of (automatically) generating image descriptions for blind and low-vision users. Elisa’s work has been supported by several Google Research Awards, the National Science Foundation, Stanford’s Human-centered AI initiative, and Stanford’s Accelerator for Learning.</p>
            <p><strong>Abstract:</strong> Developing Vision-Language Models (VLMs) that can easily translate between the linguistic and visual modality in human-like ways has many useful applications, including making visual content accessible to blind and low vision individuals, detecting misinformation, and combating visual illiteracy. While the current generation of VLMs has quickly risen to show human-level performance on many existing benchmarks, there remains a remarkable gap between these scores and how useful the models are found to be in practice. In this talk, I will present recent and ongoing work which suggests that in order to develop and understand the merit of Vision-Language Models for downstream application, we need to define tasks and evaluation metrics that assess the communicative usefulness of the generated texts. Specifically, I will focus on the challenge of generating image descriptions and argue for moving the goal post from what can be said about an image to the fundamentally pragmatic question of what should be said about it. Based on a variety of experiments with sighted and blind and low-vision participants, I will show that the pragmatic notion of contextual relevance is a core pillar of generating human-like image descriptions, provide evidence that our current tasks and evaluation tools in NLP remain unhelpful in uncovering these context effects, and present work that starts addressing this gap. Taken together, this work provides fundamental insights into how people communicate about the visual world, and shows how we can use those insights to advance VLMs for social impact, such as non-visual accessibility.</p>
        </div>
    </div>

    <h2>Organizing Committee</h2>
    <div class="committee-content">
        <h3>Faculty</h3>
        <ul class="committee-list">
            <li>
                <strong>Prof. Kai-Wei Chang</strong> - <a href="mailto:kwchang@cs.ucla.edu">kwchang@cs.ucla.edu</a>
            </li>
            <li>
                <strong>Prof. Nanyun Peng</strong> - <a href="mailto:violetpeng@cs.ucla.edu">violetpeng@cs.ucla.edu</a>
            </li>
            <li>
                <strong>Prof. Saadia Gabriel</strong> - <a href="mailto:skgabrie@cs.ucla.edu">skgabrie@cs.ucla.edu</a>
            </li>
            <li>
                <strong>Prof. Elisa Kreiss</strong> - <a href="mailto:ekreiss@ucla.edu">ekreiss@ucla.edu</a>
            </li>
        </ul>
        <h3>Students</h3>
        <ul class="committee-list">
            <li>
                <strong>Yufei Tian</strong> - <a href="mailto:yufeit@cs.ucla.edu">yufeit@cs.ucla.edu</a>
            </li>
            <li>
                <strong>Yining Hong</strong> - <a href="mailto:yufeit@cs.ucla.edu">yninghong@gmail.com</a>
            </li>
            <li>
                <strong>Tanmay Parekh</strong> - <a href="mailto:tparekh@cs.ucla.edu">tparekh@cs.ucla.edu</a>
            </li>
            <li>
                <strong>Salman Rahman</strong> -
            </li>
            <li>
                <strong>Ashima Suvarna</strong> - <a href="mailto:asuvarna31@cs.ucla.edu">asuvarna31@cs.ucla.edu</a>
            </li>
        </ul>
    </div>

    </main>

</body>

</html>